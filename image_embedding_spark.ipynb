{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c2dba4",
   "metadata": {},
   "source": [
    "## Configurazione e avvio sessione Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fefa91-69c5-4caf-bff9-41a9dd9b2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disattiva il parallelismo a livello delle librerie di calcolo per gestirlo con Spark\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4b6f62-3020-4326-aa40-9eac3bc62f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkFiles, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee67d7-ae79-4adf-96f3-879c965a0897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 12:14:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spark.executorEnv.OMP_NUM_THREADS': '1', 'spark.app.name': 'ResNetPredictNotebook', 'spark.driver.memory': '1.5g', 'spark.python.worker.reuse': 'true', 'spark.default.parallelism': '8', 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES': 'http://namenode:8088/proxy/application_1755771130202_0001', 'spark.driver.host': 'namenode', 'spark.executor.memory': '1g', 'spark.serializer.objectStreamReset': '100', 'spark.ui.proxyBase': '/proxy/application_1755771130202_0001', 'spark.submit.deployMode': 'client', 'spark.driver.port': '34487', 'spark.ui.filters': 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter', 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS': 'namenode', 'spark.driver.appUIAddress': 'http://namenode:4040', 'spark.app.id': 'application_1755771130202_0001', 'spark.executor.memoryOverhead': '512m', 'spark.driver.extraJavaOptions': '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false', 'spark.executorEnv.MKL_NUM_THREADS': '1', 'spark.executor.id': 'driver', 'spark.master': 'yarn', 'spark.rdd.compress': 'True', 'spark.executor.extraJavaOptions': '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false', 'spark.executor.instances': '1', 'spark.executorEnv.OPENBLAS_NUM_THREADS': '1', 'spark.submit.pyFiles': '', 'spark.executor.cores': '1', 'spark.app.submitTime': '1755771227567', 'spark.sql.shuffle.partitions': '8', 'spark.app.startTime': '1755771255319', 'spark.ui.showConsoleProgress': 'true'}\n"
     ]
    }
   ],
   "source": [
    "# configurazioni spark per esecuzione nel cluster yarn\n",
    "configs = {\n",
    "    \"spark.app.name\": \"ResNetPredictNotebook\",\n",
    "    \"spark.master\": \"yarn\",                    \n",
    "    \"spark.submit.deployMode\": \"client\",       \n",
    "    \"spark.executor.instances\": \"2\",        \n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.executor.memoryOverhead\": \"512m\",\n",
    "    \"spark.driver.memory\": \"1.5g\",\n",
    "    \"spark.sql.shuffle.partitions\": \"8\",\n",
    "    \"spark.default.parallelism\": \"8\",\n",
    "    \"spark.python.worker.reuse\": \"true\",\n",
    "    # passa env var agli executor\n",
    "    \"spark.executorEnv.OMP_NUM_THREADS\": \"1\",\n",
    "    \"spark.executorEnv.MKL_NUM_THREADS\": \"1\",\n",
    "    \"spark.executorEnv.OPENBLAS_NUM_THREADS\": \"1\",\n",
    "}\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "if SparkContext._active_spark_context is not None:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "    \n",
    "builder = SparkSession.builder\n",
    "for k, v in configs.items():\n",
    "    builder = builder.config(k, v)\n",
    "\n",
    "# crea la sessione spark se non disponibile\n",
    "spark = builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(dict(sc.getConf().getAll()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a013ee1b-91c4-444e-9ebc-7f57726d56d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark master: yarn\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark master:\", sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841652f",
   "metadata": {},
   "source": [
    "## Calcolo distribuito degli embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac68642-acd8-4f8f-97de-e9ddda141ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# carica il modello resnet50 e salva i pesi pre-addestrati in spark\n",
    "weights = models.ResNet50_Weights.DEFAULT\n",
    "model = models.resnet50(weights=weights)\n",
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, \"/tmp/resnet50_statedict2.pth\")\n",
    "\n",
    "sc.addFile(\"/tmp/resnet50_statedict2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33d71c-8480-4ca3-ba5a-3f9253341dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resnet_fn():\n",
    "    \"\"\"\n",
    "    Questa funzione viene eseguita una volta per python executor\n",
    "    Restituisce una funzione \"predict\"\n",
    "    \"\"\"\n",
    "    import io\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as T\n",
    "\n",
    "    #legge i pesi del modello salvati in precedenza in spark\n",
    "    local_path = SparkFiles.get(\"resnet50_statedict2.pth\")\n",
    "    # ottiene le operazioni di preprocessing per resnet50\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    preprocess = weights.transforms() \n",
    "    \n",
    "    # carica il modello con i pesi pre-addestrati\n",
    "    model = models.resnet50(weights=None)\n",
    "    state_dict = torch.load(local_path, map_location=\"cpu\", weights_only=False)\n",
    "    model.load_state_dict(state_dict)\n",
    "    #prepara feature_extractor rimuovendo ultimo layer di classificazione\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])  \n",
    "    feature_extractor.eval()\n",
    "    device = \"cpu\"\n",
    "    feature_extractor.to(device)\n",
    "\n",
    "    def predict(content_bytes_arr: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        input: numpy array 1D di bytes (batch)\n",
    "        output: numpy array 2D float32 con shape (batch, 2048)\n",
    "        \"\"\"\n",
    "        # costruisce batch tensor\n",
    "        imgs = []\n",
    "        for b in content_bytes_arr:\n",
    "            try:\n",
    "                img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                # se immagine corrotta -> sostituisce con immagine nera\n",
    "                img = Image.new(\"RGB\", (224,224))\n",
    "            imgs.append(preprocess(img))\n",
    "\n",
    "        batch = torch.stack(imgs).to(device)                    # shape (batch, 3, 224, 224)\n",
    "        # estrazione features\n",
    "        with torch.no_grad():\n",
    "            feats = feature_extractor(batch)                    # (batch, 2048, 1, 1)\n",
    "            feats = feats.reshape(feats.size(0), -1)            # (batch, 2048)\n",
    "            out = feats.cpu().numpy().astype(np.float32)        # numpy 2D array\n",
    "        return out\n",
    "\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534fdf2-32cd-4b0b-94dc-38eec3c023c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 12:16:05 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    }
   ],
   "source": [
    "# Legge le immagini da hdfs come file binari (binaryFile) e le memorizza in un dataframe\n",
    "df = spark.read.format(\"binaryFile\").load(\"hdfs:///user/hadoopuser/flickr30k_images/flickr30k_images/\").select(\"path\", \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3f7cf-2ff2-49b9-a395-f7a05ff050d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                path|\n",
      "+-------+--------------------+\n",
      "|  count|               31784|\n",
      "|   mean|                NULL|\n",
      "| stddev|                NULL|\n",
      "|    min|hdfs://namenode:9...|\n",
      "|    25%|                NULL|\n",
      "|    50%|                NULL|\n",
      "|    75%|                NULL|\n",
      "|    max|hdfs://namenode:9...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.show()\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ca2e6-8925-472c-8394-dbdfc809e7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=>                                                     (36 + 1) / 1012]"
     ]
    }
   ],
   "source": [
    "# crea la predict_batch_udf che restituisce una pandas UDF per l'inferenza\n",
    "resnet_udf = predict_batch_udf(\n",
    "    make_resnet_fn,\n",
    "    return_type=ArrayType(FloatType()),\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# Applica la predict_batch_udf alla colonna \"content\" del dataframe (bytes immagine) e genera gli embedding\n",
    "df_with_emb = df.withColumn(\"embedding\", resnet_udf(col(\"content\"))).select(\"path\", \"embedding\")\n",
    "\n",
    "# Scrive i risultati su hdfs in formatopParquet\n",
    "df_with_emb.write.mode(\"overwrite\").parquet(\"hdfs:///user/hadoopuser/flickr_image_embeddings_parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f2ecf",
   "metadata": {},
   "source": [
    "## Upload embeddings su database Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d54a3-0b69-4760-946f-a5011b2c0cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                path|           embedding|\n",
      "+--------------------+--------------------+\n",
      "|hdfs://namenode:9...|[0.0038882254, 0....|\n",
      "|hdfs://namenode:9...|[0.0, 0.0, 0.0, 0...|\n",
      "|hdfs://namenode:9...|[0.04085762, 0.0,...|\n",
      "|hdfs://namenode:9...|[0.022724897, 0.0...|\n",
      "|hdfs://namenode:9...|[0.062641725, 0.0...|\n",
      "|hdfs://namenode:9...|[1.0650856, 0.003...|\n",
      "|hdfs://namenode:9...|[0.9971608, 0.0, ...|\n",
      "|hdfs://namenode:9...|[0.0, 0.0, 0.0, 0...|\n",
      "|hdfs://namenode:9...|[0.48371467, 0.01...|\n",
      "|hdfs://namenode:9...|[0.11777613, 0.0,...|\n",
      "|hdfs://namenode:9...|[0.018268155, 0.0...|\n",
      "|hdfs://namenode:9...|[0.033272073, 0.0...|\n",
      "|hdfs://namenode:9...|[0.033873044, 0.0...|\n",
      "|hdfs://namenode:9...|[0.0, 0.0, 0.0, 0...|\n",
      "|hdfs://namenode:9...|[0.0, 0.047265064...|\n",
      "|hdfs://namenode:9...|[0.07706165, 0.0,...|\n",
      "|hdfs://namenode:9...|[0.03996246, 0.13...|\n",
      "|hdfs://namenode:9...|[2.1814766, 0.0, ...|\n",
      "|hdfs://namenode:9...|[0.0, 0.0, 0.0067...|\n",
      "|hdfs://namenode:9...|[0.2139859, 0.013...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:======================================================> (32 + 1) / 33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                path|\n",
      "+-------+--------------------+\n",
      "|  count|               31784|\n",
      "|   mean|                NULL|\n",
      "| stddev|                NULL|\n",
      "|    min|hdfs://namenode:9...|\n",
      "|    25%|                NULL|\n",
      "|    50%|                NULL|\n",
      "|    75%|                NULL|\n",
      "|    max|hdfs://namenode:9...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Carica embeddings dal parquet\n",
    "df_from_parquet = spark.read.parquet(\"hdfs:///user/hadoopuser/flickr_image_embeddings_parquet/\")\n",
    "\n",
    "# verifica dataframe\n",
    "df_from_parquet.printSchema()\n",
    "df_from_parquet.show()\n",
    "df_from_parquet.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946caa2-1fba-41bd-b127-c33f3a0c6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# verifica dimensione embeddings\n",
    "from pyspark.sql.functions import col, size\n",
    "bad_dim_count = df_from_parquet.filter(size(col(\"embedding\")) != 2048).count()\n",
    "print(bad_dim_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75949324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pymilvus import connections, utility, Collection, FieldSchema, CollectionSchema, DataType\n",
    "import numpy as np\n",
    "\n",
    "# Configurazione e creazione della Collezione su Milvus (eseguito sul driver) \n",
    "\n",
    "# Parametri di connessione e della collezione\n",
    "MILVUS_HOST = \"192.168.100.4\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "COLLECTION_NAME = \"image_embeddings_spark\"\n",
    "DIMENSION = 2048 # Dimensione dei vettori ResNet50\n",
    "\n",
    "# Connessione a Milvus dal driver\n",
    "print(f\"Connessione a Milvus su {MILVUS_HOST}:{MILVUS_PORT}\")\n",
    "connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "# Controlla se la collezione esiste già e, in caso, la elimina per rieseguire lo script\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    print(f\"Collezione '{COLLECTION_NAME}' esistente. Verrà eliminata e ricreata.\")\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "# Definisce lo schema della collezione\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"path\", dtype=DataType.VARCHAR, max_length=65535), \n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"Image embeddings generated with Spark\")\n",
    "\n",
    "# Crea la collezione\n",
    "print(f\"Creazione della collezione '{COLLECTION_NAME}'...\")\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)\n",
    "print(\"Collezione creata con successo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73dda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_partition_to_milvus(partition):\n",
    "    \"\"\"\n",
    "    Funzione per il caricamento distribuito su Milvus\n",
    "    verrà eseguita su ogni partizione (worker) del DataFrame\n",
    "    \"\"\"\n",
    "    from pymilvus import connections, Collection\n",
    "    \n",
    "    # ogni worker deve stabilire la propria connessione a Milvus\n",
    "    connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    collection = Collection(name=COLLECTION_NAME)\n",
    "    \n",
    "    rows = list(partition)\n",
    "    if not rows:\n",
    "        connections.disconnect(\"default\")\n",
    "        return\n",
    "\n",
    "    # estrae i dati in colonne\n",
    "    ids = [row[\"id\"] for row in rows]\n",
    "    paths = [row[\"path\"] for row in rows]\n",
    "    vectors = [row[\"embedding\"] for row in rows]\n",
    "    \n",
    "    data_to_insert = [ids, paths, vectors]\n",
    "    \n",
    "    # inserisce i dati della partizione\n",
    "    collection.insert(data_to_insert)\n",
    "    \n",
    "    # disconnette il worker\n",
    "    connections.disconnect(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento embeddings distribuito con Spark \n",
    "\n",
    "# aggiunge una colonna con ID univoci al dataframe letto da parquet\n",
    "df_with_ids = df_from_parquet.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# riesegue la cache del df per evitare di ricalcolarlo\n",
    "df_with_ids.cache()\n",
    "\n",
    "print(\"Inizio del caricamento distribuito dei dati su Milvus...\")\n",
    "# applica la funzione \"upload_partition_to_milvus\" a ogni partizione del dataframe\n",
    "df_with_ids.foreachPartition(upload_partition_to_milvus)\n",
    "print(\"Caricamento dati completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a52200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dell'indice e caricamento in memoria (eseguito sul driver) \n",
    "\n",
    "# assicura che tutti i dati siano stati scritti su disco in Milvus\n",
    "print(\"Flushing dei dati...\")\n",
    "collection.flush()\n",
    "print(f\"Numero totale di entità nella collezione: {collection.num_entities}\")\n",
    "\n",
    "# definisce i parametri per l'indice\n",
    "index_params = {\n",
    "    \"metric_type\": \"COSINE\",       # Metrica di distanza (cosine)\n",
    "    \"index_type\": \"IVF_FLAT\",  \n",
    "    \"params\": {\"nlist\": 1024}  \n",
    "}\n",
    "\n",
    "# crea l'indice sul campo vettoriale\n",
    "print(f\"Creazione dell'indice {index_params['index_type']}...\")\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "print(\"Indice creato con successo.\")\n",
    "\n",
    "# carica la collezione in memoria per renderla disponibile alle ricerche\n",
    "print(\"Caricamento della collezione in memoria...\")\n",
    "collection.load()\n",
    "print(\"Collezione caricata e pronta per le ricerche.\")\n",
    "\n",
    "# disconnessione dal driver\n",
    "connections.disconnect(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7647817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica che il DB risponda correttamente a una ricerca di similarità\n",
    "\n",
    "print(f\"Connessione a Milvus su {MILVUS_HOST}:{MILVUS_PORT}...\")\n",
    "connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "if not utility.has_collection(COLLECTION_NAME):\n",
    "    print(f\"ERRORE: La collezione '{COLLECTION_NAME}' non è stata trovata.\")\n",
    "else:\n",
    "    print(\"1. Conteggio delle entità\")\n",
    "    collection = Collection(name=COLLECTION_NAME)\n",
    "    collection.flush() # Assicura che gli ultimi dati scritti siano indicizzati\n",
    "    print(f\"La collezione '{COLLECTION_NAME}' contiene {collection.num_entities} entità.\")\n",
    "\n",
    "    print(\"2. Stato della collezione e dell'indice\")\n",
    "    print(f\"Indici presenti: {collection.indexes}\")\n",
    "    \n",
    "    load_state = utility.get_query_segment_info(COLLECTION_NAME)[0].state\n",
    "    print(f\"Stato di caricamento in memoria: {load_state}\")\n",
    "\n",
    "    print(\"3. Recupero di un'entità di esempio\")\n",
    "    sample_entity = collection.query(\n",
    "      expr = \"id == 0\",\n",
    "      output_fields = [\"id\", \"path\"]\n",
    "    )\n",
    "    if sample_entity:\n",
    "        print(\"Dati recuperati per id=0:\")\n",
    "        print(sample_entity)\n",
    "        sample_path = sample_entity[0]['path']\n",
    "    else:\n",
    "        print(\"Nessuna entità trovata con id=0.\")\n",
    "        sample_path = None\n",
    "\n",
    "    print(\"4. Test di ricerca per similitudine\")\n",
    "    if sample_path:\n",
    "        # input per la ricerca: il vettore appena recuperato\n",
    "        vector_to_search = collection.query(expr=f\"path == '{sample_path}'\", output_fields=[\"embedding\"])[0]['embedding']\n",
    "        \n",
    "        search_params = {\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"params\": {\"nprobe\": 10}, \n",
    "        }\n",
    "\n",
    "        # esegue la ricerca\n",
    "        results = collection.search(\n",
    "            data=[vector_to_search],          # vettore da cercare\n",
    "            anns_field=\"embedding\",           # campo su cui cercare\n",
    "            param=search_params,\n",
    "            limit=3,                          # ricerca dei 3 risultati più simili\n",
    "            output_fields=[\"path\"]            # restituisce il path delle immagini trovate\n",
    "        )\n",
    "        \n",
    "        # Se il primo risultato ha distanza 1 e corrisponde all'immagine di partenza, la ricerca funziona\n",
    "        print(\"La ricerca ha prodotto i seguenti risultati:\")\n",
    "        for i, hit in enumerate(results[0]):\n",
    "            print(f\"-- Risultato {i+1}:\")\n",
    "            print(f\"   ID: {hit.id}\")\n",
    "            print(f\"   Distanza: {hit.distance:.4f} \")\n",
    "            print(f\"   Path: {hit.entity.get('path')}\")\n",
    "            \n",
    "connections.disconnect(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "945bbdd1-0649-4ab2-b5f2-15b7fa537ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
