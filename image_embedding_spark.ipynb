{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fefa91-69c5-4caf-bff9-41a9dd9b2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4b6f62-3020-4326-aa40-9eac3bc62f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkFiles, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee67d7-ae79-4adf-96f3-879c965a0897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 12:14:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spark.executorEnv.OMP_NUM_THREADS': '1', 'spark.app.name': 'ResNetPredictNotebook', 'spark.driver.memory': '1.5g', 'spark.python.worker.reuse': 'true', 'spark.default.parallelism': '8', 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES': 'http://namenode:8088/proxy/application_1755771130202_0001', 'spark.driver.host': 'namenode', 'spark.executor.memory': '1g', 'spark.serializer.objectStreamReset': '100', 'spark.ui.proxyBase': '/proxy/application_1755771130202_0001', 'spark.submit.deployMode': 'client', 'spark.driver.port': '34487', 'spark.ui.filters': 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter', 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS': 'namenode', 'spark.driver.appUIAddress': 'http://namenode:4040', 'spark.app.id': 'application_1755771130202_0001', 'spark.executor.memoryOverhead': '512m', 'spark.driver.extraJavaOptions': '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false', 'spark.executorEnv.MKL_NUM_THREADS': '1', 'spark.executor.id': 'driver', 'spark.master': 'yarn', 'spark.rdd.compress': 'True', 'spark.executor.extraJavaOptions': '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false', 'spark.executor.instances': '1', 'spark.executorEnv.OPENBLAS_NUM_THREADS': '1', 'spark.submit.pyFiles': '', 'spark.executor.cores': '1', 'spark.app.submitTime': '1755771227567', 'spark.sql.shuffle.partitions': '8', 'spark.app.startTime': '1755771255319', 'spark.ui.showConsoleProgress': 'true'}\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"spark.app.name\": \"ResNetPredictNotebook\",\n",
    "    \"spark.master\": \"yarn\",                    \n",
    "    \"spark.submit.deployMode\": \"client\",       \n",
    "    \"spark.executor.instances\": \"2\",        \n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.executor.memoryOverhead\": \"512m\",\n",
    "    \"spark.driver.memory\": \"1.5g\",\n",
    "    \"spark.sql.shuffle.partitions\": \"8\",\n",
    "    \"spark.default.parallelism\": \"8\",\n",
    "    \"spark.python.worker.reuse\": \"true\",\n",
    "    # passa env var agli executor: OMP/MKL/OPENBLAS threads a 1\n",
    "    \"spark.executorEnv.OMP_NUM_THREADS\": \"1\",\n",
    "    \"spark.executorEnv.MKL_NUM_THREADS\": \"1\",\n",
    "    \"spark.executorEnv.OPENBLAS_NUM_THREADS\": \"1\",\n",
    "}\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "if SparkContext._active_spark_context is not None:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "    \n",
    "builder = SparkSession.builder\n",
    "for k, v in configs.items():\n",
    "    builder = builder.config(k, v)\n",
    "\n",
    "# crea la session (se non disponibile, la crea)\n",
    "spark = builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(dict(sc.getConf().getAll()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a013ee1b-91c4-444e-9ebc-7f57726d56d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark master: yarn\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark master:\", sc.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac68642-acd8-4f8f-97de-e9ddda141ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "weights = models.ResNet50_Weights.DEFAULT\n",
    "model = models.resnet50(weights=weights)\n",
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, \"/tmp/resnet50_statedict2.pth\")\n",
    "\n",
    "sc.addFile(\"/tmp/resnet50_statedict2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b33d71c-8480-4ca3-ba5a-3f9253341dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resnet_fn():\n",
    "    \"\"\"\n",
    "    Questa funzione viene eseguita una volta per python worker (executor).\n",
    "    Deve restituire una funzione `predict` che riceve un np.ndarray (batch) e ritorna\n",
    "    un np.ndarray 2D (batch, embedding_dim).\n",
    "    \"\"\"\n",
    "    import io\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as T\n",
    "\n",
    "    #legge i pesi del modello salvati in precedenza in spark\n",
    "    local_path = SparkFiles.get(\"resnet50_statedict2.pth\")\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    preprocess = weights.transforms() \n",
    "    \n",
    "    # carica il modello e prepara feature_extractor rimuovendo ultimo layer\n",
    "    model = models.resnet50(weights=None)\n",
    "    state_dict = torch.load(local_path, map_location=\"cpu\", weights_only=False)\n",
    "    model.load_state_dict(state_dict)\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])  \n",
    "    feature_extractor.eval()\n",
    "    device = \"cpu\"\n",
    "    feature_extractor.to(device)\n",
    "\n",
    "    def predict(content_bytes_arr: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        content_bytes_arr: 1-d numpy array of bytes-like objects (batch,)\n",
    "        ritorna: 2-d numpy array float32 shape (batch, 2048)\n",
    "        \"\"\"\n",
    "        # costruisci batch tensor\n",
    "        imgs = []\n",
    "        for b in content_bytes_arr:\n",
    "            try:\n",
    "                img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                # se immagine corrotta -> sostituisci con black image (o gestisci come preferisci)\n",
    "                img = Image.new(\"RGB\", (224,224))\n",
    "            imgs.append(preprocess(img))\n",
    "\n",
    "        xs = torch.stack(imgs).to(device)  # shape (batch, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            feats = feature_extractor(xs)                     # (batch, 2048, 1, 1)\n",
    "            feats = feats.reshape(feats.size(0), -1)          # (batch, 2048)\n",
    "            out = feats.cpu().numpy().astype(np.float32)      # numpy 2D\n",
    "        return out\n",
    "\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6534fdf2-32cd-4b0b-94dc-38eec3c023c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 12:16:05 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    }
   ],
   "source": [
    "# Legge file binari (binaryFile): colonne incluse: path, modificationTime, length, content (binary)\n",
    "df = spark.read.format(\"binaryFile\").load(\"hdfs:///user/hadoopuser/flickr30k_images/flickr30k_images/\").select(\"path\", \"content\")\n",
    "\n",
    "#df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cb3f7cf-2ff2-49b9-a395-f7a05ff050d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                path|\n",
      "+-------+--------------------+\n",
      "|  count|               31784|\n",
      "|   mean|                NULL|\n",
      "| stddev|                NULL|\n",
      "|    min|hdfs://namenode:9...|\n",
      "|    25%|                NULL|\n",
      "|    50%|                NULL|\n",
      "|    75%|                NULL|\n",
      "|    max|hdfs://namenode:9...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ca2e6-8925-472c-8394-dbdfc809e7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=>                                                     (36 + 1) / 1012]"
     ]
    }
   ],
   "source": [
    "# crea il predict_batch_udf: ritorna una colonna ArrayType(FloatType()) (2D --> array per riga)\n",
    "resnet_udf = predict_batch_udf(\n",
    "    make_resnet_fn,\n",
    "    return_type=ArrayType(FloatType()),   # 2D: (batch, dim) -> ogni riga riceverà un array<float> (embedding)\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# Applichiamo il predict UDF sulla colonna 'content' (bytes)\n",
    "df_with_emb = df.withColumn(\"embedding\", resnet_udf(col(\"content\"))).select(\"path\", \"embedding\")\n",
    "\n",
    "# Scriviamo in Parquet (scalabile)\n",
    "df_with_emb.write.mode(\"overwrite\").parquet(\"hdfs:///user/hadoopuser/flickr_image_embeddings_parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd597c5-4516-4b11-9fdf-30b6c35c877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test con funz predict_batch_udf che calcola solo costanti\n",
    "'''\n",
    "def make_constant():\n",
    "    import numpy as np\n",
    "    def predict(batch):\n",
    "        # batch è np.ndarray di oggetti; ritorniamo (B, 4) per semplicità\n",
    "        B = len(batch)\n",
    "        out = np.zeros((B,4), dtype=np.float32) + 0.5\n",
    "        return out\n",
    "    return predict\n",
    "\n",
    "const_udf = predict_batch_udf(make_constant, return_type=ArrayType(FloatType()), batch_size=2)\n",
    "#df.show()\n",
    "#df = spark.read.format(\"binaryFile\").load(\"hdfs:///some/small/images/\").limit(10).select(\"path\",\"content\")\n",
    "df2 = df.withColumn(\"emb\", const_udf(col(\"content\")))\n",
    "df2.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2d54a3-0b69-4760-946f-a5011b2c0cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                path|           embedding|\n",
      "+--------------------+--------------------+\n",
      "|hdfs://namenode:9...|[0.0038882254, 0....|\n",
      "|hdfs://namenode:9...|[0.0, 0.0, 0.0, 0...|\n",
      "|hdfs://namenode:9...|[0.04085762, 0.0,...|\n",
      "|hdfs://namenode:9...|[0.022724897, 0.0...|\n",
      "|hdfs://namenode:9...|[0.062641725, 0.0...|\n",
      "|hdfs://namenode:9...|[1.0650856, 0.003...|\n",
      "|hdfs://namenode:9...|[0.9971608, 0.0, ...|\n",
      "|hdfs://namenode:9...|[0.0, 0.0, 0.0, 0...|\n",
      "|hdfs://namenode:9...|[0.48371467, 0.01...|\n",
      "|hdfs://namenode:9...|[0.11777613, 0.0,...|\n",
      "|hdfs://namenode:9...|[0.018268155, 0.0...|\n",
      "|hdfs://namenode:9...|[0.033272073, 0.0...|\n",
      "|hdfs://namenode:9...|[0.033873044, 0.0...|\n",
      "|hdfs://namenode:9...|[0.0, 0.0, 0.0, 0...|\n",
      "|hdfs://namenode:9...|[0.0, 0.047265064...|\n",
      "|hdfs://namenode:9...|[0.07706165, 0.0,...|\n",
      "|hdfs://namenode:9...|[0.03996246, 0.13...|\n",
      "|hdfs://namenode:9...|[2.1814766, 0.0, ...|\n",
      "|hdfs://namenode:9...|[0.0, 0.0, 0.0067...|\n",
      "|hdfs://namenode:9...|[0.2139859, 0.013...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:======================================================> (32 + 1) / 33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                path|\n",
      "+-------+--------------------+\n",
      "|  count|               31784|\n",
      "|   mean|                NULL|\n",
      "| stddev|                NULL|\n",
      "|    min|hdfs://namenode:9...|\n",
      "|    25%|                NULL|\n",
      "|    50%|                NULL|\n",
      "|    75%|                NULL|\n",
      "|    max|hdfs://namenode:9...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# Carica embeddings dal parquet\n",
    "df_from_parquet = spark.read.parquet(\"hdfs:///user/hadoopuser/test2_image_embeddings_parquet_test2/\")\n",
    "\n",
    "# Controlla schema\n",
    "df_from_parquet.printSchema()\n",
    "# dovrebbe mostrare: path: string, embedding: array<float>\n",
    "df_from_parquet.show()\n",
    "df_from_parquet.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c946caa2-1fba-41bd-b127-c33f3a0c6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, size, expr, sqrt\n",
    "bad_dim_count = df_from_parquet.filter(size(col(\"embedding\")) != 2048).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ef7989-53f8-422b-85bb-a7caf533ad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(bad_dim_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75949324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pymilvus import connections, utility, Collection, FieldSchema, CollectionSchema, DataType\n",
    "import numpy as np\n",
    "\n",
    "# Configurazione e Creazione della Collezione su Milvus (eseguito sul driver) \n",
    "\n",
    "# Parametri di connessione e della collezione\n",
    "MILVUS_HOST = \"192.168.100.4\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "COLLECTION_NAME = \"image_embeddings_spark\"\n",
    "DIMENSION = 2048 # Dimensione dei vettori ResNet50\n",
    "\n",
    "# Connessione a Milvus dal driver\n",
    "print(f\"Connessione a Milvus su {MILVUS_HOST}:{MILVUS_PORT}\")\n",
    "connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "# Controlla se la collezione esiste già e, in caso, la elimina per rieseguire lo script\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    print(f\"Collezione '{COLLECTION_NAME}' esistente. Verrà eliminata e ricreata.\")\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "# Definisce lo schema della collezione\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"path\", dtype=DataType.VARCHAR, max_length=65535), \n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"Image embeddings generated with Spark\")\n",
    "\n",
    "# Crea la collezione\n",
    "print(f\"Creazione della collezione '{COLLECTION_NAME}'...\")\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)\n",
    "print(\"Collezione creata con successo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73dda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione della Funzione per il Caricamento Distribuito su Milvus\n",
    "\n",
    "# Questa funzione verrà eseguita su ogni partizione (worker) del DataFrame\n",
    "def upload_partition_to_milvus(partition):\n",
    "    # Importa le librerie necessarie all'interno della funzione\n",
    "    from pymilvus import connections, Collection\n",
    "    \n",
    "    # Ogni worker deve stabilire la propria connessione a Milvus\n",
    "    connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    collection = Collection(name=COLLECTION_NAME)\n",
    "    \n",
    "    rows = list(partition)\n",
    "    if not rows:\n",
    "        connections.disconnect(\"default\")\n",
    "        return\n",
    "\n",
    "    # Estrai i dati in colonne\n",
    "    ids = [row[\"id\"] for row in rows]\n",
    "    paths = [row[\"path\"] for row in rows]\n",
    "    vectors = [row[\"embedding\"] for row in rows]\n",
    "    \n",
    "    data_to_insert = [ids, paths, vectors]\n",
    "    \n",
    "    # Inserisci i dati della partizione\n",
    "    collection.insert(data_to_insert)\n",
    "    \n",
    "    # Disconnetti il worker\n",
    "    connections.disconnect(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento Distribuito con Spark \n",
    "\n",
    "# Aggiunge una colonna con ID univoci al DataFrame letto da Parquet\n",
    "df_with_ids = df_from_parquet.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# riesegue la cache del df per evitare di ricalcolarlo\n",
    "df_with_ids.cache()\n",
    "\n",
    "print(\"Inizio del caricamento distribuito dei dati su Milvus...\")\n",
    "# Applica la funzione 'upload_partition_to_milvus' a ogni partizione del DataFrame\n",
    "df_with_ids.foreachPartition(upload_partition_to_milvus)\n",
    "print(\"Caricamento dati completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a52200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dell'Indice e Caricamento in Memoria (eseguito sul driver) \n",
    "\n",
    "# Assicura che tutti i dati siano stati scritti su disco in Milvus\n",
    "print(\"Flushing dei dati...\")\n",
    "collection.flush()\n",
    "print(f\"Numero totale di entità nella collezione: {collection.num_entities}\")\n",
    "\n",
    "# Definisce i parametri per l'indice\n",
    "index_params = {\n",
    "    \"metric_type\": \"COSINE\",       # Metrica di distanza (cosine)\n",
    "    \"index_type\": \"IVF_FLAT\",  # Indice standard per bilanciare velocità e accuratezza\n",
    "    \"params\": {\"nlist\": 1024}  # Numero di cluster, da calibrare in base ai dati\n",
    "}\n",
    "\n",
    "# Crea l'indice sul campo vettoriale\n",
    "print(f\"Creazione dell'indice {index_params['index_type']}...\")\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "print(\"Indice creato con successo.\")\n",
    "\n",
    "# Carica la collezione in memoria per renderla disponibile alle ricerche\n",
    "print(\"Caricamento della collezione in memoria...\")\n",
    "collection.load()\n",
    "print(\"Collezione caricata e pronta per le ricerche.\")\n",
    "\n",
    "# Disconnettiti dal driver\n",
    "connections.disconnect(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7647817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICA che il DB risponda correttamente a una ricerca di similarità\n",
    "\n",
    "# Connessione a Milvus \n",
    "print(f\"Connessione a Milvus su {MILVUS_HOST}:{MILVUS_PORT}...\")\n",
    "connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "# Verifica il Numero di Vettori \n",
    "print(\"\\n1. Conteggio delle entità\")\n",
    "if not utility.has_collection(COLLECTION_NAME):\n",
    "    print(f\"ERRORE: La collezione '{COLLECTION_NAME}' non è stata trovata.\")\n",
    "else:\n",
    "    collection = Collection(name=COLLECTION_NAME)\n",
    "    collection.flush() # Assicura che gli ultimi dati scritti siano indicizzati\n",
    "    print(f\"La collezione '{COLLECTION_NAME}' contiene {collection.num_entities} entità.\")\n",
    "\n",
    "    # Controlla lo Stato della Collezione e dell'Indice \n",
    "    print(\"\\n2. Stato della collezione e dell'indice\")\n",
    "    print(f\"La collezione ha un indice?\")\n",
    "    print(f\"Indici presenti: {collection.indexes}\")\n",
    "    \n",
    "    # Verifica lo stato di caricamento\n",
    "    load_state = utility.get_query_segment_info(COLLECTION_NAME)[0].state\n",
    "    print(f\"Stato di caricamento in memoria: {load_state}\")\n",
    "\n",
    "\n",
    "    # Recupera un'Entità di Esempio (Query) \n",
    "    print(\"\\n3. Recupero di un'entità di esempio\")\n",
    "    # Recuperiamo l'entità con id = 0 (la prima inserita da Spark)\n",
    "    sample_entity = collection.query(\n",
    "      expr = \"id == 0\",\n",
    "      output_fields = [\"id\", \"path\"]\n",
    "    )\n",
    "    if sample_entity:\n",
    "        print(\"Dati recuperati per id=0:\")\n",
    "        print(sample_entity)\n",
    "        sample_path = sample_entity[0]['path']\n",
    "    else:\n",
    "        print(\"Nessuna entità trovata con id=0.\")\n",
    "        sample_path = None\n",
    "\n",
    "    # Esegui una Ricerca per Similitudine \n",
    "    print(\"\\n4. Test di ricerca per similitudine\")\n",
    "    if sample_path:\n",
    "        # Prendiamo il vettore dell'entità che abbiamo appena recuperato\n",
    "        vector_to_search = collection.query(expr=f\"path == '{sample_path}'\", output_fields=[\"embedding\"])[0]['embedding']\n",
    "        \n",
    "        # Impostiamo i parametri della ricerca\n",
    "        search_params = {\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"params\": {\"nprobe\": 10}, \n",
    "        }\n",
    "\n",
    "        # Eseguiamo la ricerca\n",
    "        results = collection.search(\n",
    "            data=[vector_to_search],          # Il vettore da cercare\n",
    "            anns_field=\"embedding\",           # Il campo su cui cercare\n",
    "            param=search_params,\n",
    "            limit=3,                          # Vogliamo i 3 risultati più simili\n",
    "            output_fields=[\"path\"]            # Vogliamo ottenere il path delle immagini trovate\n",
    "        )\n",
    "\n",
    "        # Mostriamo i risultati\n",
    "        print(\"La ricerca ha prodotto i seguenti risultati:\")\n",
    "        for i, hit in enumerate(results[0]):\n",
    "            print(f\"  Risultato {i+1}:\")\n",
    "            print(f\"    - ID: {hit.id}\")\n",
    "            print(f\"    - Distanza: {hit.distance:.4f} \")\n",
    "            print(f\"    - Path: {hit.entity.get('path')}\")\n",
    "            \n",
    "        print(\"\\nSe il primo risultato ha distanza 1 e corrisponde all'immagine di partenza, la ricerca funziona\")\n",
    "\n",
    "# Disconnessione \n",
    "connections.disconnect(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "945bbdd1-0649-4ab2-b5f2-15b7fa537ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
